{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle: Plant Pathology 2021 - FGVC8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pytorch-lightning torchmetrics -q\n",
    "! pip list | grep torch\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Checking what data do we have available and what is the labels distribution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsu to see what is the data location\n",
    "! ls /kaggle/input/plant-pathology-2021-fgvc8 -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking in the training dataset table, what colums and what is the data representation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "base_path = '/kaggle/input/plant-pathology-2021-fgvc8'\n",
    "path_csv = os.path.join(base_path, 'train.csv')\n",
    "train_data = pd.read_csv(path_csv)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each image can have multiple labels so lets check what is the mos common label count...\n",
    "\n",
    "*The target classes, a space delimited list of all diseases found in the image.\n",
    "Unhealthy leaves with too many diseases to classify visually will have the complex class, and may also have a subset of the diseases identified.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data['nb_classes'] = [len(lbs.split(\" \")) for lbs in train_data['labels']]\n",
    "lb_hist = dict(zip(range(10), np.bincount(train_data['nb_classes'])))\n",
    "pprint(lb_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Browse the label distribution, enrolling all labels in the dataset, so in case an image has two labels both are used in this stat..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "labels_all = list(itertools.chain(*[lbs.split(\" \") for lbs in train_data['labels']]))\n",
    "\n",
    "ax = sns.countplot(y=labels_all, orient='v')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some stat for labels combinations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_unique = set(labels_all)\n",
    "print(f\"unique labels: {labels_unique}\")\n",
    "train_data['labels_sorted'] = [\" \".join(sorted(lbs.split(\" \"))) for lbs in train_data['labels']]\n",
    "\n",
    "labels_combine = {}\n",
    "for comb in train_data['labels_sorted']:\n",
    "    labels_combine[comb] = labels_combine.get(comb, 0) + 1\n",
    "\n",
    "show_counts = '\\n'.join(sorted(f'\\t{k}: {v}' for k, v in labels_combine.items()))\n",
    "print(f\"unique combinations: \\n\" + show_counts)\n",
    "print(f\"total: {sum(labels_combine.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add visualisation over each case, so five a few examples per labe combination..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nb_samples = 6\n",
    "n, m = len(np.unique(train_data['labels_sorted'])), nb_samples,\n",
    "fig, axarr = plt.subplots(nrows=n, ncols=m, figsize=(m * 2, n * 2))\n",
    "for ilb, (lb, df_) in enumerate(train_data.groupby('labels_sorted')):\n",
    "    img_names = list(df_['image'])\n",
    "    for i in range(m):\n",
    "        img_name = img_names[i]\n",
    "        img = plt.imread(os.path.join(base_path, f\"train_images/{img_name}\"))\n",
    "        axarr[ilb, i].imshow(img)\n",
    "        if i == 0:\n",
    "            axarr[ilb, i].set_title(f\"{lb} #{len(df_)}\")\n",
    "        axarr[ilb, i].set_xticks([])\n",
    "        axarr[ilb, i].set_yticks([])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & DataModule\n",
    "\n",
    "Creating standard PyTorch dataset to define how the data shall be loaded and set representations.\n",
    "We define the sample pair as:\n",
    " - RGB image\n",
    " - one-hot labels encoding\n",
    "\n",
    "A DataModule standardizes the training, val, test splits, data preparation and transforms.\n",
    "The main advantage is consistent data splits, data preparation and transforms across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from kaggle_plantpatho.data import PlantPathologyDataset\n",
    "\n",
    "dataset = PlantPathologyDataset()\n",
    "\n",
    "# quick view\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "for i in range(9):\n",
    "    img, lb = dataset[i]\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add also a simplified version, where we keep only complex label for multi-label cases and the true label for all others..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_plantpatho.data import PlantPathologySimpleDataset\n",
    "\n",
    "dataset = PlantPathologySimpleDataset()\n",
    "\n",
    "# quick view\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "for i in range(9):\n",
    "    img, lb = dataset[i]\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"label: {lb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataModule include creating training and validation dataset with given split and feading it to particular data loaders..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "from kaggle_plantpatho.data import PlantPathologyDM\n",
    "\n",
    "dm = PlantPathologyDM()\n",
    "dm.setup()\n",
    "print(dm.num_classes)\n",
    "\n",
    "# quick view\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "for imgs, lbs in dm.train_dataloader():\n",
    "    lb_hist= dict(zip(range(10), np.bincount(lbs)))\n",
    "    print(f'batch labels: {lb_hist}')\n",
    "    print(f'image size: {imgs[0].shape}')\n",
    "    for i in range(5):\n",
    "        ax = fig.add_subplot(1, 5, i + 1, xticks=[], yticks=[])\n",
    "        # print(np.rollaxis(imgs[i].numpy(), 0, 3).shape)\n",
    "        ax.imshow(np.rollaxis(imgs[i].numpy(), 0, 3))\n",
    "        ax.set_title(f\"label: {lbs[i]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm2 = PlantPathologyDM(full=True)\n",
    "dm2.setup()\n",
    "\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "for imgs, lbs in dm2.train_dataloader():\n",
    "    print(f'batch labels: {torch.sum(lbs, axis=0)}')\n",
    "    print(f'image size: {imgs[0].shape}')\n",
    "    for i in range(5):\n",
    "        ax = fig.add_subplot(1, 5, i + 1, xticks=[], yticks=[])\n",
    "        # print(np.rollaxis(imgs[i].numpy(), 0, 3).shape)\n",
    "        ax.imshow(np.rollaxis(imgs[i].numpy(), 0, 3))\n",
    "        ax.set_title(lbs[i])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model\n",
    "\n",
    "We start with some stanrd CNN models taken from torch vision.\n",
    "Then we define Ligthning module including training and validation step and configure optimizer/schedular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_plantpatho.models import LitResnet, LitPlantPathology\n",
    "\n",
    "# see: https://pytorch.org/vision/stable/models.html\n",
    "net = LitResnet(arch='resnet50', num_classes=dm.num_classes)\n",
    "# print(net)\n",
    "model = LitPlantPathology(model=net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We use Pytorch Lightning which allow us to drop all the boilet plate code and simplify all training just to use/call Trainer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = pl.loggers.CSVLogger(save_dir='logs/', name=model.arch)\n",
    "\n",
    "# ==============================\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    # fast_dev_run=True,\n",
    "    gpus=1,\n",
    "    # callbacks=[cb_ckpt],\n",
    "    logger=logger,\n",
    "    max_epochs=10,\n",
    "    precision=16,\n",
    "    accumulate_grad_batches=8,\n",
    "    val_check_interval=0.25,\n",
    "    progress_bar_refresh_rate=1,\n",
    "    weights_summary='top',\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "\n",
    "# trainer.tune(model, datamodule=dm)\n",
    "trainer.fit(model=model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick visualization of the training process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(f'{trainer.logger.log_dir}/metrics.csv')\n",
    "print(metrics.head())\n",
    "\n",
    "aggreg_metrics = []\n",
    "agg_col = \"epoch\"\n",
    "for i, dfg in metrics.groupby(agg_col):\n",
    "    agg = dict(dfg.mean())\n",
    "    agg[agg_col] = i\n",
    "    aggreg_metrics.append(agg)\n",
    "\n",
    "df_metrics = pd.DataFrame(aggreg_metrics)\n",
    "df_metrics[['train_loss', 'valid_loss']].plot(grid=True, legend=True, xlabel=agg_col)\n",
    "df_metrics[['valid_f1', 'valid_acc', 'train_acc']].plot(grid=True, legend=True, xlabel=agg_col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}