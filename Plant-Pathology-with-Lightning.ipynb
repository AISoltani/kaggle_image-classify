{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle: Plant Pathology 2021 - FGVC8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pytorch-lightning torchmetrics -q\n",
    "! pip list | grep torch\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Checking what data do we have available and what is the labels distribution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsu to see what is the data location\n",
    "! ls /kaggle/input/plant-pathology-2021-fgvc8 -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking in the training dataset table, what colums and what is the data representation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "base_path = '/kaggle/input/plant-pathology-2021-fgvc8'\n",
    "path_csv = os.path.join(base_path, 'train.csv')\n",
    "train_data = pd.read_csv(path_csv)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each image can have multiple labels so lets check what is the mos common label count...\n",
    "\n",
    "*The target classes, a space delimited list of all diseases found in the image.\n",
    "Unhealthy leaves with too many diseases to classify visually will have the complex class, and may also have a subset of the diseases identified.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data['nb_classes'] = [len(lbs.split(\" \")) for lbs in train_data['labels']]\n",
    "lb_hist = dict(zip(range(10), np.bincount(train_data['nb_classes'])))\n",
    "pprint(lb_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Browse the label distribution, enrolling all labels in the dataset, so in case an image has two labels both are used in this stat..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "labels_all = list(itertools.chain(*[lbs.split(\" \") for lbs in train_data['labels']]))\n",
    "\n",
    "ax = sns.countplot(y=labels_all, orient='v')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some stat for labels combinations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_unique = set(labels_all)\n",
    "print(f\"unique labels: {labels_unique}\")\n",
    "train_data['labels_sorted'] = [\" \".join(sorted(lbs.split(\" \"))) for lbs in train_data['labels']]\n",
    "\n",
    "labels_combine = {}\n",
    "for comb in train_data['labels_sorted']:\n",
    "    labels_combine[comb] = labels_combine.get(comb, 0) + 1\n",
    "\n",
    "show_counts = '\\n'.join(sorted(f'\\t{k}: {v}' for k, v in labels_combine.items()))\n",
    "print(f\"unique combinations: \\n\" + show_counts)\n",
    "print(f\"total: {sum(labels_combine.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add visualisation over each case, so five a few examples per labe combination..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nb_samples = 6\n",
    "n, m = len(np.unique(train_data['labels_sorted'])), nb_samples,\n",
    "fig, axarr = plt.subplots(nrows=n, ncols=m, figsize=(m * 2, n * 2))\n",
    "for ilb, (lb, df_) in enumerate(train_data.groupby('labels_sorted')):\n",
    "    img_names = list(df_['image'])\n",
    "    for i in range(m):\n",
    "        img_name = img_names[i]\n",
    "        img = plt.imread(os.path.join(base_path, f\"train_images/{img_name}\"))\n",
    "        axarr[ilb, i].imshow(img)\n",
    "        if i == 0:\n",
    "            axarr[ilb, i].set_title(f\"{lb} #{len(df_)}\")\n",
    "        axarr[ilb, i].set_xticks([])\n",
    "        axarr[ilb, i].set_yticks([])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & DataModule\n",
    "\n",
    "Creating standard PyTorch dataset to define how the data shall be loaded and set representations.\n",
    "We define the sample pair as:\n",
    " - RGB image\n",
    " - one-hot lable encding\n",
    "\n",
    "A DataModule standardizes the training, val, test splits, data preparation and transforms.\n",
    "The main advantage is consistent data splits, data preparation and transforms across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PlantPathologyDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path_csv: str = os.path.join(base_path, 'train.csv'),\n",
    "        path_img_dir: str = os.path.join(base_path, 'train_images'),\n",
    "        transforms = None,\n",
    "        mode: str = 'train',\n",
    "        split: float = 0.8,\n",
    "    ):\n",
    "        self.path_img_dir = path_img_dir\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "\n",
    "        self.data = pd.read_csv(path_csv)\n",
    "        labels_all = list(itertools.chain(*[lbs.split(\" \") for lbs in self.data['labels']]))\n",
    "        self.labels_unique = sorted(set(labels_all))\n",
    "        self.labels_lut = {lb: i for i, lb in enumerate(self.labels_unique)}\n",
    "        self.num_classes = len(self.labels_unique)\n",
    "        # shuffle data\n",
    "        self.data = self.data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        # split dataset\n",
    "        assert 0.0 <= split <= 1.0\n",
    "        frac = int(split * len(self.data))\n",
    "        self.data = self.data[:frac] if mode == 'train' else self.data[frac:]\n",
    "        self.img_names = list(self.data['image'])\n",
    "        self.labels = list(self.data['labels'])\n",
    "\n",
    "    def to_one_hot(self, labels: str) -> tuple:\n",
    "        one_hot = [0] * len(self.labels_unique)\n",
    "        for lb in labels.split(\" \"):\n",
    "            one_hot[self.labels_lut[lb]] = 1\n",
    "        return tuple(one_hot)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        img_path = os.path.join(self.path_img_dir, self.img_names[idx])\n",
    "        assert os.path.isfile(img_path)\n",
    "        label = self.labels[idx]\n",
    "        img = plt.imread(img_path)\n",
    "\n",
    "        # augmentation\n",
    "        if self.transforms:\n",
    "            img = self.transforms(Image.fromarray(img))\n",
    "        label = self.to_one_hot(label)\n",
    "        return img, torch.tensor(label)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "# ==============================\n",
    "# ==============================\n",
    "\n",
    "dataset = PlantPathologyDataset()\n",
    "\n",
    "# quick view\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "for i in range(9):\n",
    "    img, lb = dataset[i]\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add also a simplified version, where we keep only complex label for multi-label cases and the true label for all others..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantPathologySimpleDataset(PlantPathologyDataset):\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        img, label = super().__getitem__(idx)\n",
    "        if torch.sum(label) > 1:\n",
    "            label = self.labels_lut['complex']\n",
    "        else:\n",
    "            label = torch.argmax(label)\n",
    "        return img, int(label)\n",
    "\n",
    "# ==============================\n",
    "# ==============================\n",
    "\n",
    "dataset = PlantPathologySimpleDataset()\n",
    "\n",
    "# quick view\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "for i in range(9):\n",
    "    img, lb = dataset[i]\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"label: {lb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define some standard image augmentaion procedures and color normalizations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "\n",
    "TRAIN_TRANSFORM = T.Compose([\n",
    "    T.Resize(512),\n",
    "    T.RandomPerspective(),\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    # T.Normalize([0.431, 0.498,  0.313], [0.237, 0.239, 0.227]),  # custom\n",
    "])\n",
    "\n",
    "VALID_TRANSFORM = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    # T.Normalize([0.431, 0.498,  0.313], [0.237, 0.239, 0.227]),  # custom\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataModule include creating training and validation dataset with given split and feading it to particular data loaders..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mproc\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PlantPathologyDM(pl.LightningDataModule):\n",
    "    dataset_cls = PlantPathologySimpleDataset\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path_csv: str = os.path.join(base_path, 'train.csv'),\n",
    "        path_img_dir: str = os.path.join(base_path, 'train_images'),\n",
    "        batch_size: int = 128,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.path_csv = path_csv\n",
    "        self.path_img_dir = path_img_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers if num_workers is not None else mproc.cpu_count()\n",
    "        self.train_dataset = None\n",
    "        self.valid_dataset = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        assert self.train_dataset and self.valid_dataset\n",
    "        return max(self.train_dataset.num_classes, self.valid_dataset.num_classes)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = self.dataset_cls(self.path_csv, self.path_img_dir, mode='train', transforms=TRAIN_TRANSFORM)\n",
    "        print(f\"training dataset: {len(self.train_dataset)}\")\n",
    "        self.valid_dataset = self.dataset_cls(self.path_csv, self.path_img_dir, mode='valid', transforms=VALID_TRANSFORM)\n",
    "        print(f\"validation dataset: {len(self.valid_dataset)}\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        pass\n",
    "\n",
    "# ==============================\n",
    "# ==============================\n",
    "\n",
    "dm = PlantPathologyDM()\n",
    "dm.setup()\n",
    "print(dm.num_classes)\n",
    "\n",
    "# quick view\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "for imgs, lbs in dm.train_dataloader():\n",
    "    lb_hist= dict(zip(range(10), np.bincount(lbs)))\n",
    "    print(f'batch labels: {lb_hist}')\n",
    "    print(f'image size: {imgs[0].shape}')\n",
    "    for i in range(5):\n",
    "        ax = fig.add_subplot(1, 5, i + 1, xticks=[], yticks=[])\n",
    "        # print(np.rollaxis(imgs[i].numpy(), 0, 3).shape)\n",
    "        ax.imshow(np.rollaxis(imgs[i].numpy(), 0, 3))\n",
    "        ax.set_title(f\"label: {lbs[i]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm2 = PlantPathologyDM()\n",
    "dm2.dataset_cls = PlantPathologyDataset\n",
    "dm2.setup()\n",
    "\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "for imgs, lbs in dm2.train_dataloader():\n",
    "    print(f'batch labels: {torch.sum(lbs, axis=0)}')\n",
    "    print(f'image size: {imgs[0].shape}')\n",
    "    for i in range(5):\n",
    "        ax = fig.add_subplot(1, 5, i + 1, xticks=[], yticks=[])\n",
    "        # print(np.rollaxis(imgs[i].numpy(), 0, 3).shape)\n",
    "        ax.imshow(np.rollaxis(imgs[i].numpy(), 0, 3))\n",
    "        ax.set_title(lbs[i])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model\n",
    "\n",
    "We start with some stanrd CNN models taken from torch vision.\n",
    "Then we define Ligthning module including training and validation step and configure optimizer/schedular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class LitResnet(nn.Module):\n",
    "    def __init__(self, arch: str, pretrained: bool = True, num_classes: int = 6):\n",
    "        super().__init__()\n",
    "        self.arch = arch\n",
    "        self.num_classes = num_classes\n",
    "        self.model = torchvision.models.__dict__[arch](pretrained=pretrained)\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class LitPlantPathology(pl.LightningModule):\n",
    "    def __init__(self, model, lr: float = 1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.arch = self.model.arch\n",
    "        self.num_classes = self.model.num_classes\n",
    "        self.train_accuracy = torchmetrics.Accuracy()\n",
    "        self.val_accuracy = torchmetrics.Accuracy()\n",
    "        self.val_f1_score = torchmetrics.F1(self.num_classes)\n",
    "        self.learn_rate = lr\n",
    "        self.loss_fn = F.cross_entropy\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.model(x))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", self.train_accuracy(y_hat, y), prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"valid_loss\", loss, prog_bar=False)\n",
    "        self.log(\"valid_acc\", self.val_accuracy(y_hat, y), prog_bar=True)\n",
    "        self.log(\"valid_f1\", self.val_f1_score(y_hat, y), prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learn_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, self.trainer.max_epochs, 0)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "# ==============================\n",
    "# ==============================\n",
    "\n",
    "# see: https://pytorch.org/vision/stable/models.html\n",
    "net = LitResnet(arch='resnet50', num_classes=dm.num_classes)\n",
    "# print(net)\n",
    "model = LitPlantPathology(model=net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We use Pytorch Lightning which allow us to drop all the boilet plate code and simplify all training just to use/call Trainer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = pl.loggers.CSVLogger(save_dir='logs/', name=model.arch)\n",
    "\n",
    "# ==============================\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    # fast_dev_run=True,\n",
    "    gpus=1,\n",
    "    # callbacks=[cb_ckpt],\n",
    "    logger=logger,\n",
    "    max_epochs=10,\n",
    "    precision=16,\n",
    "    accumulate_grad_batches=8,\n",
    "    val_check_interval=0.25,\n",
    "    progress_bar_refresh_rate=1,\n",
    "    weights_summary='top',\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "\n",
    "# trainer.tune(model, datamodule=dm)\n",
    "trainer.fit(model=model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick visualization of the training process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(f'{trainer.logger.log_dir}/metrics.csv')\n",
    "print(metrics.head())\n",
    "\n",
    "aggreg_metrics = []\n",
    "agg_col = \"epoch\"\n",
    "for i, dfg in metrics.groupby(agg_col):\n",
    "    agg = dict(dfg.mean())\n",
    "    agg[agg_col] = i\n",
    "    aggreg_metrics.append(agg)\n",
    "\n",
    "df_metrics = pd.DataFrame(aggreg_metrics)\n",
    "df_metrics[['train_loss', 'valid_loss']].plot(grid=True, legend=True, xlabel=agg_col)\n",
    "df_metrics[['valid_f1', 'valid_acc', 'train_acc']].plot(grid=True, legend=True, xlabel=agg_col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
